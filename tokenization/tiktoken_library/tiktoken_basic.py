import tiktoken

def test_tokenization(string):
    encoding = tiktoken.get_encoding("o200k_base")
    encoded_string = encoding.encode(string)
    #token_bytes = [encoding.decode_single_token_bytes(token) for token in encoded_string]
    token_bytes = encoding.decode_tokens_bytes(encoded_string)

    print(f"No of tokens: {len(encoded_string)}")
    print(f"Encoded string: {encoded_string}")
    print(f"Decoded string: {token_bytes}")

#test_string = "‚Ç¨  üòÑ  ‰Ω†"

#test_string_array = ["hello", "Hello", "HELLO"]
# test_string_array = ["ChatGPT", "chatgpt", "chat gpt"]
# for value in test_string_array:
#     test_tokenization(value)
#     print("\n")

test_string_in_english = "Sunlight, still soft and golden, filters through the leaves as I begin my morning walk, a daily ritual that grounds my day before the world truly awakens. The air, crisp and carrying the scent of damp earth and blooming jasmine, feels clean and revitalizing, a stark contrast to the city's later bustle."
test_string_in_hindi = "‡§∏‡•Å‡§¨‡§π ‡§ï‡•Ä ‡§∏‡•à‡§∞ ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§§‡•á ‡§π‡•Ä ‡§∏‡•Ç‡§∞‡§ú ‡§ï‡•Ä ‡§ï‡•ã‡§Æ‡§≤ ‡§î‡§∞ ‡§∏‡•Å‡§®‡§π‡§∞‡•Ä ‡§ï‡§ø‡§∞‡§£‡•á‡§Ç ‡§™‡§§‡•ç‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§∏‡•á ‡§õ‡§®‡§ï‡§∞ ‡§Ü‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ ‡§Ø‡§π ‡§è‡§ï ‡§¶‡•à‡§®‡§ø‡§ï ‡§¶‡§ø‡§®‡§ö‡§∞‡•ç‡§Ø‡§æ ‡§π‡•à ‡§ú‡•ã ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§ú‡§æ‡§ó‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§Æ‡•á‡§∞‡•á ‡§¶‡§ø‡§® ‡§ï‡•ã ‡§è‡§ï ‡§®‡§à ‡§¶‡§ø‡§∂‡§æ ‡§¶‡•á‡§§‡•Ä ‡§π‡•à‡•§ ‡§π‡§µ‡§æ ‡§§‡§æ‡§ú‡§º‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§â‡§∏‡§Æ‡•á‡§Ç ‡§®‡§Æ ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§î‡§∞ ‡§ñ‡§ø‡§≤‡§§‡•á ‡§ö‡§Æ‡•á‡§≤‡•Ä ‡§ï‡•Ä ‡§ñ‡•Å‡§∂‡§¨‡•Ç ‡§ò‡•Å‡§≤‡•Ä ‡§π‡•Å‡§à ‡§π‡•à, ‡§ú‡•ã ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§î‡§∞ ‡§∏‡•ç‡§´‡•Ç‡§∞‡•ç‡§§‡§ø‡§¶‡§æ‡§Ø‡§ï ‡§Æ‡§π‡§∏‡•Ç‡§∏ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à, ‡§∂‡§π‡§∞ ‡§ï‡•Ä ‡§¨‡§æ‡§¶ ‡§ï‡•Ä ‡§≠‡§æ‡§ó‡§¶‡•å‡§°‡§º ‡§∏‡•á ‡§¨‡§ø‡§≤‡§ï‡•Å‡§≤ ‡§Ö‡§≤‡§ó‡•§"

test_tokenization(test_string_in_english)
print("\n")
test_tokenization(test_string_in_hindi)