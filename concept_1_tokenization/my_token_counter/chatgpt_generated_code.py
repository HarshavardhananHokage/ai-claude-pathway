### This is code generated by chatgpt for the same code that you wrote.
### This is how senior python devs write code as per gpt.
### Good for learning.

import os
from dataclasses import dataclass
from enum import Enum

import tiktoken
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()


class TokenStrategy(Enum):
    TIKTOKEN = "tiktoken"
    API = "api"


@dataclass(frozen=True)
class ModelConfig:
    input_cost_per_million: float
    context_window: int
    token_strategy: TokenStrategy


MODEL_CONFIGS = {
    "gpt-4o": ModelConfig(
        input_cost_per_million=2.50,
        context_window=128_000,
        token_strategy=TokenStrategy.TIKTOKEN,
    ),
    "gpt-5": ModelConfig(
        input_cost_per_million=1.25,
        context_window=400_000,  # assumed
        token_strategy=TokenStrategy.API,
    ),
}


class TokenPlusCostCounter:
    def __init__(self, model: str):
        if model not in MODEL_CONFIGS:
            raise ValueError(f"Unsupported model: {model}")

        self.model = model
        self.config = MODEL_CONFIGS[model]
        self.client = OpenAI(api_key=os.getenv("OPEN_AI_KEY"))

    def count_tokens(self, prompt: str) -> int:
        if self.config.token_strategy is TokenStrategy.API:
            return self._count_tokens_via_api(prompt)
        return self._count_tokens_via_tiktoken(prompt)

    def _count_tokens_via_tiktoken(self, prompt: str) -> int:
        encoding = tiktoken.encoding_for_model(self.model)
        return len(encoding.encode(prompt))

    def _count_tokens_via_api(self, prompt: str) -> int:
        response = self.client.responses.create(
            model=self.model,
            input=prompt,
            max_output_tokens=0,  # important: avoid extra billing
        )
        return response.usage.input_tokens

    def calculate_cost(self, token_count: int) -> float:
        return self.config.input_cost_per_million * (token_count / 1_000_000)

    def check_context_window(self, token_count: int) -> str:
        usage_ratio = token_count / self.config.context_window

        if usage_ratio < 0.5:
            return "You are safe. Continue prompting."
        return "Warning! Prompt exceeds 50% of context window."

    def analyze_prompt(self, prompt: str) -> None:
        token_count = self.count_tokens(prompt)
        cost = self.calculate_cost(token_count)

        print(f"Model: {self.model}")
        print(f"Prompt: {prompt}")
        print(f"Tokens used: {token_count}")
        print(f"Estimated input cost: ${cost:.8f}")
        print(self.check_context_window(token_count))
        print("-" * 40)


if __name__ == "__main__":
    checker_4o = TokenPlusCostCounter("gpt-4o")
    checker_4o.analyze_prompt("Hello. How are you?")

    checker_5 = TokenPlusCostCounter("gpt-5")
    checker_5.analyze_prompt("Hello. How are you?")
